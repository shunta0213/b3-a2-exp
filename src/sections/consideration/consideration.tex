\documentclass[../../../main]{subfiles}
\begin{document}

\section{考察}\label{sec:consideration}

\subsection{乱数生成}

\subsubsection{線形合同法の問題点}
線形合同法の問題点は、周期が短いことである。
周期の長いパラメータが提案されており、そのうちいくつかの値で乱数を生成しそのヒストグラムを作成し、
乱数の分布をみる。
乱数の生成数を$N$として$a=75, c=74, m=2^{16}+1$としたとき、そのヒストグラムを図\ref{fig:lcg-zx}に示す。
$N=2^{16}$のときは一様に分布していることがわかる。
一方で、適当な値として$s=101, c=49, m=20040213$とすると、図\ref{fig:lcg-rand}のようになる。
$N=2^{16}$のときも一様でなく出やすい値、出にくい値がでてしまうことがわかる。

線形合同法で周期の長いパラメータを発見しようとすると、
$a$、$c$、$m$の3つのパラメータを考える必要がある。
$m$が大きければ周期は大きくなる傾向にある一方で、$a$、$m$を適切に決定するためには計算量が必要となる。
全探索しようとすれば、$\mathcal{O}(n^3)$の計算量が必要となる。
これは現実的ではなく、問題点として挙げられる。

\input{src/figures/lcg-plot/lcg-plot.tex}

\clearpage
\subsubsection{その他乱数生成法とベンチマーク}
Go言語にて次の
\begin{itemize}
	\item math/rand pkg \footnote{
		      Go言語の標準パッケージである。
		      シードを指定することで完全に再現可能な乱数を生成することができる。
		      生成アルゴリズムはALFG(Additive Lagged Fibonacci Generator)のバリエーションとのことである。
		      現在、Go 1.22にてChaCha8を使用したmath/rand/v2が開発されている。\cite{salsa20}\cite{go-math-rand-v2}
	      }
	\item crypto/rand pkg \footnote{
		      Go言語の標準パッケージである。
		      暗号的に安全な乱数を生成することができる。
		      環境に応じたノイズをシードとして使用し、出力が予測不可能である。
	      }
	\item Middle Square Method
	\item Linear Congruential Generator
\end{itemize}
についてベンチマークを行い、それぞれの特性や性能を比較する。
なお、LCGのパラメータは$m=2^{16} + 1$, $a=75$, $c=74$とした。\footnote{
	ZX81と呼ばれるコンピュータにて使われているパラメータである。
}
3秒間のベンチマークを行った結果を表\ref{tab:random-bench}に示す。
LCGが最もメモリ消費量が少なく、最も高速であることがわかる。
MSMはメモリ消費量、速度ともにLCGより劣る。
MSMが遅い理由としては、Go言語においてある数から中央の部分を取り出す処理が遅いということが考えられる。
math/randの場合は、LCGよりは遅く、メモリ消費量も少ない。
crypto/randは、メモリ消費量が最も多く、速度もLCGより遅い。
これは暗号学的に安全な乱数を生成するために、より多くの計算を行うためである。

\paragraph{Middle Square Method (MSM)}
平方菜中法は、乱数生成アルゴリズムの一つである。
乱数生成アルゴリズムの中でも最も単純なものの一つであり、
乱数のシードを2乗し、その中央の部分を取り出すことで乱数を生成する。
\input{src/tables/bench-random.tex}

\subsubsection{疑似乱数の活用}
擬似乱数法の応用、特に暗号学的擬似乱数についてまとめる。
暗号学的擬似乱数とは、乱数生成法と出力が既知である場合にも出力が予測できないような乱数生成法のことである。
代表的な方法としては、暗号学的ハッシュ関数と呼ばれる一方向性の関数を使用する方法である。
暗号学的ハッシュ関数には、
\begin{itemize}
	\item 原像計算困難性
	      ハッシュ値からもとのメッセージを計算することが困難であること
	\item 強衝突耐性
	      ある2つ以上のメッセージから同じハッシュが得られることが事実上不可能であること
\end{itemize}
といった性質が求められる。
代表的な生成法は、
\begin{itemize}
	\item SHA-1
	\item SHA-256
	\item MD-5
\end{itemize}
などがあげられる。
SHA-1は、2017年にGoogleがSHA-1の衝突を発見した。\cite{google-sha1-collision}
そのため今では使用が推奨されていない。
これらの関数は、例えばパスワードのハッシュ化や、ファイル・DockerイメージなどのChecksum、暗号通貨などに使用される。
パスワードでは原像計算困難性が大事である。
Checksumでは強衝突耐性と、すなわちハッシュが一致すればファイルが一致することが保証されることが重要である。
暗号通貨では、強衝突耐性とこれらハッシュ関数に必要な計算量、加えて情報の圧縮などが重要な役割を果たす。

\subsubsection{乱数によるブラウン運動のシミュレーション}
\input{src/sections/consideration/src/brownian-sim.tex}

\subsection{重回帰分析}

\subsubsection{実験における重回帰分析の改良}
実験では一次関数を用いて重回帰分析を行った。
より次数を上げた多項式回帰を行うことで、より適切なモデルを構築することができる。
$n$を1から4まで変化させたときの多項式回帰の結果を図\ref{fig:polynomial-regression}に示す。
\input{src/figures/polynominal-regression/polynominal-regression.tex}
実際に$n$が増加するにつれて理想的な直線に近づいていることがわかる。
$n$を1から24まで変化させたとき、決定係数の値を表\ref{tab:polynomial-regression}に示す。
$n$を増加させていけば、決定係数が1に近づいていく。
一方で、$n$を増加させすぎると与えられたデータに対して過剰に適合してしまうことがある。
この場合、未知のデータに対しては学習データに比べて損失が大きくなる。
これを過学習などという。
防ぐためには、学習に用いるデータとテストに用いるデータを分け、学習のデータでまずは損失が少なくなるように係数を決定し、
学習とは別にテストデータでモデルにとって未知のデータに対する損失を計算することで過学習を防ぐ適切な次数を選択することができる。
今回はデータ数が少ないため、そこまでは行わない。
\input{src/tables/poly-nominal-data.tex}

\subsubsection{NBA2022-2023シーズンにおける重回帰分析}
NBA(National Basketball Association)の2022-2023シーズンにおける選手のスタッツのデータを用いて重回帰分析を行う。
データは、Vivo Vinco氏のKaggleのデータセットを用いる。\footnote{
	\url{https://www.kaggle.com/datasets/vivovinco/20222023-nba-player-stats-regular}.
	CC-BY 4.0 ライセンス
}
以下の情報を含む。 Rk,
Player, Pos, Age, Tm, G, GS, MP, FG, FGA, FG\%, 3P, 3PA, 3P\%, 2P, 2PA, 2P\%, eFG\%, FT, FTA, FT\%, ORB, DRB, TRB, AST, STL, BLK, TOV, PF, PTS, TRB, AST, STL, BLK, TOV, PF, PTS
\footnote{
	Rk : Rank,
	Player : Player's name,
	Pos : Position,
	Age : Player's age,
	Tm : Team,
	G : Games played,
	GS : Games started,
	MP : Minutes played per game,
	FG : Field goals per game,
	FGA : Field goal attempts per game,
	FG\% : Field goal percentage,
	3P : 3-point field goals per game,
	3PA : 3-point field goal attempts per game,
	3P\% : 3-point field goal percentage,
	2P : 2-point field goals per game,
	2PA : 2-point field goal attempts per game,
	2P\% : 2-point field goal percentage, ,
	eFG\% : Effective field goal percentage,
	FT : Free throws per game,
	FTA : Free throw attempts per game,
	FT\% : Free throw percentage,
	ORB : Offensive rebounds per game,
	DRB : Defensive rebounds per game,
	TRB : Total rebounds per game,
	AST : Assists per game,
	STL : Steals per game,
	BLK : Blocks per game,
	TOV : Turnovers per game,
	PF : Personal fouls per game,
	PTS : Points per gameTRB : Total rebounds per game,
	AST : Assists per game,
	STL : Steals per game,
	BLK : Blocks per game,
	TOV : Turnovers per game,
	PF : Personal fouls per game,
	PTS : Points per game
}
これらのデータの中から、まず
Age, Pos, MP, FG, FGA, FG\%, 3P, 3PA, 3P\%, FT, FTA, FT\%を説明変数とし、
PTSを目的変数として重回帰分析を行う。
今回は学習データとテストデータをわけ、学習データでモデルを構築し、テストデータでモデルの性能を評価する。
決定係数は$0.9998950394227705$となり非常に高い精度で予測ができていることがわかる。
予測した値と実際の値の比較を図\ref{fig:actual-vs-predicted-PTS}に示す。
シュートに関するほぼ全ての統計量を説明変数とし、
得点を目的変数としているからのかなり高い精度で予測できたと考えられる。
\input{src/figures/nba-regression/actual-vs-predicted-PTS.tex}

一方で、MP, Age, STL, BLKを説明変数としPTSを目的変数とすると、
決定係数は$0.787534156007302$となり上記の場合に比べて低くなっている。
これは、FGに関する情報、FTに関する情報などを説明変数とせず、PTSに直接関係のないような
AgeやMP、STL、BLKを説明変数としているためであると考えられる。
\input{src/figures/nba-regression/actual-vs-predicted-PTS-2.tex}

PTS以外の全ての統計量を説明変数としPTSを目的変数とすると、決定係数は$0.999872848991546$となりはじめの場合に比べてやや高くなっている。
一方で、得点を説明するのはAge, Pos, MP, FG, FGA, FG\%, 3P, 3PA, 3P\%, FT, FTA, FT\%などで十分と考えられ、
それ以外の統計量はただ不要なパラメータを増やし計算量を増やすだけである。
したがって、上記のように目的変数に対してどういう説明変数が関連しているか適切に判断し選択することが大事である。


\end{document}
